{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iport Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train = pd.read_csv('raw_data/train_data.csv')\n",
    "cc_test = pd.read_csv('raw_data/test_data.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(dataframe, verbose=True):\n",
    "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "  start_memory = dataframe.memory_usage().sum() / 1024**2\n",
    "  for col in dataframe.columns:\n",
    "    col_type = dataframe[col].dtypes\n",
    "    if col_type in numerics:\n",
    "      c_min = dataframe[col].min()\n",
    "      c_max = dataframe[col].max()\n",
    "      if str(col_type)[:3] == 'int':\n",
    "        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "          dataframe[col] = dataframe[col].astype(np.int8)\n",
    "        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "          dataframe[col] = dataframe[col].astype(np.int16)\n",
    "        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "          dataframe[col] = dataframe[col].astype(np.int32)\n",
    "        elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "          dataframe[col] = dataframe[col].astype(np.int64)\n",
    "      else:\n",
    "        if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "          dataframe[col] = dataframe[col].astype(np.float32)\n",
    "        else:\n",
    "          dataframe[col] = dataframe[col].astype(np.float64)\n",
    "  end_memory = dataframe.memory_usage().sum() / 1024**2\n",
    "  print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_memory, 100 * (start_memory - end_memory) / start_memory)) if verbose else print('Reduced to {:5.2f}'.format(end_memory))\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 352.24 Mb (50.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "cc_train = reduce_mem_usage(cc_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = cc_train.copy()\n",
    "test_df = cc_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375734, 246)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import iqr\n",
    "\n",
    "def remove_outliers_tukey(data, alpha=1.5):\n",
    "    '''\n",
    "    Remove outliers using Tukey's method with the interquartile range (IQR).\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy array or pandas dataframe): The data to remove outliers from.\n",
    "    alpha (float): The sensitivity parameter, which determines the range to consider outliers.\n",
    "                   A value of 1.5 is the default, which is a commonly used value.\n",
    "    \n",
    "    Returns:\n",
    "    numpy array or pandas dataframe: The data with outliers removed.\n",
    "    '''\n",
    "    # Select only the numerical columns\n",
    "    num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data_num = data[num_cols]\n",
    "    \n",
    "    # Compute the first and third quartiles\n",
    "    q1, q3 = np.percentile(data_num, [25, 75])\n",
    "    \n",
    "    # Compute the interquartile range (IQR)\n",
    "    iqr_val = iqr(data_num)\n",
    "    \n",
    "    # Compute the range outside of which data points are considered outliers\n",
    "    outlier_range = (q1 - alpha * iqr_val, q3 + alpha * iqr_val)\n",
    "    \n",
    "    # Identify the outliers and remove them\n",
    "    outliers = (data_num < outlier_range[0]) | (data_num > outlier_range[1])\n",
    "    data_num_no_outliers = data_num[~outliers]\n",
    "    \n",
    "    # Merge the numerical columns back into the original data frame\n",
    "    data_no_outliers = pd.concat([data_num_no_outliers, data.select_dtypes(exclude=[np.number])], axis=1)\n",
    "    return data_no_outliers\n",
    "\n",
    "data_with_removed_outliers = remove_outliers_tukey(train_df)\n",
    "data_with_removed_outliers.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"contest-tmp2m-14d__tmp2m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_feature(train, test):\n",
    "    scale = 14\n",
    "    train.loc[:,'lat']=round(train.lat,scale)\n",
    "    train.loc[:,'lon']=round(train.lon,scale)\n",
    "    test.loc[:,'lat']=round(test.lat,scale)\n",
    "    test.loc[:,'lon']=round(test.lon,scale)\n",
    "    \n",
    "    train_and_test = pd.concat([train, test], axis=0)\n",
    "    train_and_test['loc_group'] = train_and_test.groupby(['lat', 'lon']).ngroup()\n",
    "    print(f'{train_and_test.loc_group.nunique()} unique locations')\n",
    "    \n",
    "    train = train_and_test.iloc[:len(train)]\n",
    "    test = train_and_test.iloc[len(train):].drop(target, axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def cat_encode(train, test):\n",
    "    # encoding the categorical feature in the train and test data set\n",
    "    # using OneHotEncoder\n",
    "    ohe = OneHotEncoder()\n",
    "    train_encoded = ohe.fit_transform(train[['climateregions__climateregion']])\n",
    "    test_encoded = ohe.transform(test[['climateregions__climateregion']])\n",
    "    \n",
    "    train = train.drop(['climateregions__climateregion'], axis=1)\n",
    "    test = test.drop(['climateregions__climateregion'], axis=1)\n",
    "    \n",
    "    train_encoded = pd.DataFrame(train_encoded.toarray(), columns=ohe.get_feature_names_out(['climateregions__climateregion']))\n",
    "    test_encoded = pd.DataFrame(test_encoded.toarray(), columns=ohe.get_feature_names_out(['climateregions__climateregion']))\n",
    "    \n",
    "    train = pd.concat([train, train_encoded], axis=1)\n",
    "    test = pd.concat([test, test_encoded], axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "    \n",
    "def fill_na_rows(dataset):\n",
    "    # Find the columns with missing values\n",
    "    columns_with_missing_values = dataset.columns[dataset.isnull().any()].tolist()\n",
    "    \n",
    "    # Impute the missing values with the mean value of that column\n",
    "    for col in columns_with_missing_values:\n",
    "        dataset[col].fillna(dataset[col].mean(), inplace=True)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "def create_new_feat(dataset):\n",
    "    dataset['year']=pd.DatetimeIndex(dataset['startdate']).year \n",
    "    dataset['month']=pd.DatetimeIndex(dataset['startdate']).month \n",
    "    dataset['day']=pd.DatetimeIndex(dataset['startdate']).day\n",
    "    return dataset\n",
    "\n",
    "def feature_engineering(origin_train, origin_test):\n",
    "    train, test = origin_train, origin_test\n",
    "    train = fill_na_rows(train)\n",
    "    train = create_new_feat(train)\n",
    "    test = create_new_feat(test)\n",
    "    train, test = cat_encode(train, test)\n",
    "    irrelevant_cols = ['index', 'startdate','contest-tmp2m-14d__tmp2m', 'climateregions__climateregion']\n",
    "    features = [col for col in train.columns if col not in irrelevant_cols]\n",
    "    #features = [col for col in train.columns]\n",
    "    X = train[features]\n",
    "    X_test = test[features]\n",
    "    y = train['contest-tmp2m-14d__tmp2m']\n",
    "    # Initialize the scaler\n",
    "    #scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the train data\n",
    "    #scaler.fit(X)\n",
    "\n",
    "    # Transform the train data\n",
    "    #X_train_scaled = scaler.transform(X)\n",
    "\n",
    "    # Transform the test data\n",
    "    #X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X, y, X_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_copy = cc_test.copy()\n",
    "X, y, X_test = feature_engineering(cc_train.copy(), cc_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_correlated(df, threshold):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    reduced_corr_matrix = corr_matrix.mask(mask)\n",
    "    features_to_drop = [c for c in reduced_corr_matrix.columns if any(reduced_corr_matrix[c] > threshold)]\n",
    "    return features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = identify_correlated(cc_train, .80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_feature = ['index', 'contest-tmp2m-14d__tmp2m']\n",
    "features_to_drop_v1 = [ele for ele in features_to_drop if ele not in remove_feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_train_reduced = pd.DataFrame(X.drop(features_to_drop_v1, axis=1))\n",
    "cc_test_reduced = pd.DataFrame(X_test.drop(features_to_drop_v1, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_preds(xgboost_preds, light_gbm_preds):\n",
    "    combined_preds = np.mean([xgboost_preds, lightgbm_preds], axis=0)\n",
    "    return combined_preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_tts, y_train, y_test = train_test_split(cc_train_reduced, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chewr\\OneDrive\\Documents\\ML labs\\Datathon-WiDS-2023\\copied.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5000\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39moob_score\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m regr_rfr \u001b[39m=\u001b[39m RandomForestRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m regr_rfr\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# make predictions on the test data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chewr/OneDrive/Documents/ML%20labs/Datathon-WiDS-2023/copied.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred_rfr \u001b[39m=\u001b[39m regr_rfr\u001b[39m.\u001b[39mpredict(X_test_tts)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    479\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    480\u001b[0m )(\n\u001b[0;32m    481\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    482\u001b[0m         t,\n\u001b[0;32m    483\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    484\u001b[0m         X,\n\u001b[0;32m    485\u001b[0m         y,\n\u001b[0;32m    486\u001b[0m         sample_weight,\n\u001b[0;32m    487\u001b[0m         i,\n\u001b[0;32m    488\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    489\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    490\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    491\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    494\u001b[0m )\n\u001b[0;32m    496\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1314\u001b[0m     \u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \n\u001b[0;32m   1316\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1343\u001b[0m         X,\n\u001b[0;32m   1344\u001b[0m         y,\n\u001b[0;32m   1345\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1346\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1348\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chewr\\Downloads\\New folder\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': 5000,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'sqrt',\n",
    "    'bootstrap': True,\n",
    "    'oob_score': True\n",
    "}\n",
    "regr_rfr = RandomForestRegressor(**params)\n",
    "regr_rfr.fit(X_train, y_train)\n",
    "# make predictions on the test data\n",
    "y_pred_rfr = regr_rfr.predict(X_test_tts)\n",
    "\n",
    "# calculate the RMSE\n",
    "rmse = mean_squared_error(y_test, y_pred_rfr,squared=False)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_pred = regr_rfr.predict(cc_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_test_copy[target] = cc_test_pred\n",
    "cc_test_copy[[target,\"index\"]].to_csv(\"rfrpredictions.csv\",index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "            'base_score': 0.5, \n",
    "            'booster': 'gbtree',\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'n_estimators': 6000,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "            'verbosity': -1,\n",
    "            'gpu_id': 0\n",
    "        }\n",
    "        reg_xgb = xgb.XGBRegressor(**params)\n",
    "\n",
    "        reg_xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test_tts, y_test)])\n",
    "\n",
    "        y_pred_xgb = reg_xgb.predict(X_test_tts)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=15)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f'  Value: {trial.value:.5f}')\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "best_params = trial.params\n",
    "reg_xgb = xgb.XGBRegressor(**best_params)\n",
    "reg_xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test_tts, y_test)])\n",
    "y_pred_xgb = reg_xgb.predict(X_test_tts)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "cc_test_pred = reg_xgb.predict(cc_test_reduced)\n",
    "cc_test_copy[target] = cc_test_pred\n",
    "cc_test_copy[[target,\"index\"]].to_csv(\"xgbpredictions.csv\",index = False)\n",
    "print(\"Finished training and fitting, created xgbpredictions.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit a robust linear regression model\n",
    "rlm_model = sm.RLM(y_train, X_train, M=sm.robust.norms.Hampel())\n",
    "rlm_results = rlm_model.fit(scale_est=sm.robust.scale.HuberScale())\n",
    "\n",
    "# Print the summary of the model\n",
    "print(rlm_results.summary())\n",
    "\n",
    "# Get the predicted values on the test set\n",
    "y_pred = rlm_results.predict(X_test_tts)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2ca5a9fb0c3d125eedde15df7126b6856a1ba2886b4386b1f8634238a9578f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
